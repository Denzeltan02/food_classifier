{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "026b01b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m,\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import os, json, time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# --------- CONFIG ---------\n",
    "DATA_DIR = \"dataset_split\"            # <-- change me to your dataset root\n",
    "MODEL_NAME = \"efficientnet_b0\"  # choices: efficientnet_b0, resnet50, resnet18\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LR = 3e-4\n",
    "UNFREEZE_AT = 3               # epoch to unfreeze backbone and fine-tune\n",
    "NUM_WORKERS = 2\n",
    "OUT_DIR = \"artifacts\"\n",
    "SEED = 42\n",
    "\n",
    "# Reproducibility\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b7440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(img_size=224):\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std  = [0.229, 0.224, 0.225]\n",
    "    train_tfms = transforms.Compose([\n",
    "        transforms.Resize(int(img_size*1.15)),\n",
    "        transforms.RandomResizedCrop(img_size, scale=(0.7, 1.0)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    eval_tfms = transforms.Compose([\n",
    "        transforms.Resize(int(img_size*1.15)),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ])\n",
    "    return train_tfms, eval_tfms\n",
    "\n",
    "def build_loaders(data_dir, img_size, batch_size, workers):\n",
    "    train_tfms, eval_tfms = get_transforms(img_size)\n",
    "    train_ds = datasets.ImageFolder(Path(data_dir)/\"train\", transform=train_tfms)\n",
    "    val_ds   = datasets.ImageFolder(Path(data_dir)/\"val\",   transform=eval_tfms)\n",
    "\n",
    "    train_ld = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  num_workers=workers, pin_memory=True)\n",
    "    val_ld   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, num_workers=workers, pin_memory=True)\n",
    "    return train_ds, val_ds, train_ld, val_ld\n",
    "\n",
    "train_ds, val_ds, train_ld, val_ld = build_loaders(DATA_DIR, IMG_SIZE, BATCH_SIZE, NUM_WORKERS)\n",
    "classes = train_ds.classes\n",
    "print(f\"Classes ({len(classes)}): {classes}\")\n",
    "\n",
    "# Save index mapping\n",
    "with open(Path(OUT_DIR)/\"idx_to_class.json\",\"w\") as f:\n",
    "    json.dump({i:c for c,i in train_ds.class_to_idx.items()}, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0b1dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_name, num_classes, pretrained=True):\n",
    "    name = model_name.lower()\n",
    "    if name == \"efficientnet_b0\":\n",
    "        m = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT if pretrained else None)\n",
    "        in_features = m.classifier[1].in_features\n",
    "        m.classifier[1] = nn.Linear(in_features, num_classes)\n",
    "    elif name == \"resnet50\":\n",
    "        m = models.resnet50(weights=models.ResNet50_Weights.DEFAULT if pretrained else None)\n",
    "        in_features = m.fc.in_features\n",
    "        m.fc = nn.Linear(in_features, num_classes)\n",
    "    else:\n",
    "        m = models.resnet18(weights=models.ResNet18_Weights.DEFAULT if pretrained else None)\n",
    "        in_features = m.fc.in_features\n",
    "        m.fc = nn.Linear(in_features, num_classes)\n",
    "    return m\n",
    "\n",
    "def freeze_backbone(model, model_name):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    if \"efficientnet\" in model_name:\n",
    "        for p in model.classifier.parameters():\n",
    "            p.requires_grad = True\n",
    "    else:\n",
    "        for p in model.fc.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "def unfreeze_all(model):\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "model = build_model(MODEL_NAME, num_classes=len(classes)).to(device)\n",
    "freeze_backbone(model, MODEL_NAME)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR)\n",
    "scheduler = optim.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type == \"cuda\"))\n",
    "\n",
    "best_acc, best_path = 0.0, str(Path(OUT_DIR)/\"best.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866f308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device, scaler=None):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    for images, targets in loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, targets)\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "    return running_loss/total, correct/total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    loss_sum, correct, total = 0.0, 0, 0\n",
    "    all_preds, all_targets = [], []\n",
    "    for images, targets in loader:\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        logits = model(images)\n",
    "        loss = criterion(logits, targets)\n",
    "        loss_sum += loss.item() * images.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(targets.cpu().numpy())\n",
    "    avg_loss = loss_sum/total\n",
    "    acc = correct/total\n",
    "    import numpy as np\n",
    "    y_pred = np.concatenate(all_preds) if all_preds else np.array([])\n",
    "    y_true = np.concatenate(all_targets) if all_targets else np.array([])\n",
    "    return avg_loss, acc, y_true, y_pred\n",
    "\n",
    "history = {\"train_loss\":[], \"train_acc\":[], \"val_loss\":[], \"val_acc\":[]}\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    if epoch == UNFREEZE_AT:\n",
    "        unfreeze_all(model)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=LR*0.3)  # smaller LR for fine-tune\n",
    "        print(f\"Unfroze backbone at epoch {epoch}\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    tr_loss, tr_acc = train_one_epoch(model, train_ld, criterion, optimizer, device, scaler)\n",
    "    va_loss, va_acc, y_true, y_pred = evaluate(model, val_ld, criterion, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    history[\"train_loss\"].append(tr_loss); history[\"train_acc\"].append(tr_acc)\n",
    "    history[\"val_loss\"].append(va_loss);   history[\"val_acc\"].append(va_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{EPOCHS} | \"\n",
    "          f\"train loss {tr_loss:.4f} acc {tr_acc:.3f} | \"\n",
    "          f\"val loss {va_loss:.4f} acc {va_acc:.3f} | \"\n",
    "          f\"{(time.time()-t0):.1f}s\")\n",
    "\n",
    "    if va_acc > best_acc:\n",
    "        best_acc = va_acc\n",
    "        torch.save({\"model\":model.state_dict(),\n",
    "                    \"model_name\":MODEL_NAME,\n",
    "                    \"classes\":classes,\n",
    "                    \"img_size\":IMG_SIZE}, best_path)\n",
    "\n",
    "print(f\"\\nBest val acc: {best_acc:.3f}. Saved to {best_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e25a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(history[\"train_acc\"])+1)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, history[\"train_acc\"], label=\"train_acc\")\n",
    "plt.plot(epochs, history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Accuracy\"); plt.title(\"Accuracy\"); plt.legend(); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, history[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(epochs, history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss\"); plt.legend(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335fff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload best checkpoint for evaluation\n",
    "ckpt = torch.load(best_path, map_location=device)\n",
    "best_model = build_model(ckpt[\"model_name\"], len(ckpt[\"classes\"])).to(device)\n",
    "best_model.load_state_dict(ckpt[\"model\"])\n",
    "\n",
    "val_loss, val_acc, y_true, y_pred = evaluate(best_model, val_ld, criterion, device)\n",
    "print(f\"Validation accuracy (best): {val_acc:.4f}\\n\")\n",
    "\n",
    "# Detailed report\n",
    "target_names = ckpt[\"classes\"]\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(target_names))))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "disp.plot(ax=ax, xticks_rotation=90, colorbar=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4428256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def get_infer_transform(img_size=224):\n",
    "    mean=[0.485,0.456,0.406]; std=[0.229,0.224,0.225]\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize(int(img_size*1.15)),\n",
    "        transforms.CenterCrop(img_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean,std),\n",
    "    ])\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_image(img_path, ckpt_path=best_path, device=device):\n",
    "    ckpt = torch.load(ckpt_path, map_location=device)\n",
    "    model = build_model(ckpt[\"model_name\"], len(ckpt[\"classes\"])).to(device)\n",
    "    model.load_state_dict(ckpt[\"model\"]); model.eval()\n",
    "    tfm = get_infer_transform(ckpt[\"img_size\"])\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    x = tfm(img).unsqueeze(0).to(device)\n",
    "    logits = model(x)\n",
    "    probs = torch.softmax(logits, dim=1)[0]\n",
    "    conf, idx = probs.max(0)\n",
    "    return ckpt[\"classes\"][idx.item()], float(conf.item())\n",
    "\n",
    "# Example:\n",
    "# pred, score = predict_image(\"path/to/your_image.jpg\")\n",
    "# print(pred, score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
